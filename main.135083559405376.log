[1731156349] Log start
[1731156349] Cmd: /home/ubuntu/ai_program/lama/llama.cpp/./main -m /home/ubuntu/ai_program/lama/llama.cpp/models/7B/llama-2-7b-chat.Q3_K_M.gguf --threads 8 -c 2048 -ins --in-prefix "" "" --color
[1731156349] main: build = 1232 (feea179)
[1731156349] main: seed  = 1731156349
[1731156349] main: llama backend init
[1731156349] main: load the model and apply lora adapter, if any
[1731156357] warming up the model with an empty run
[1731156369] 
[1731156369] system_info: n_threads = 8 / 4 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
[1731156369] add_bos: 1
[1731156369] tokenize the prompt
[1731156369] prompt: ""
[1731156369] tokens: [ '':1 ]
[1731156369] n_ctx: 2048
[1731156369] recalculate the cached logits (check): embd_inp.empty() false, n_matching_session_tokens 0, embd_inp.size() 1, session_tokens.size() 0, embd_inp.size() 1
[1731156369] inp_pfx: [ '':1, ' ':29871, '':13, '':13, '##':2277, '#':29937, ' Inst':2799, 'ruction':4080, ':':29901, '':13, '':13 ]
[1731156369] inp_sfx: [ ' ':29871, '':13, '':13, '##':2277, '#':29937, ' Response':13291, ':':29901, '':13, '':13 ]
[1731156369] main: interactive mode on.
[1731156369] Reverse prompt: '### Instruction:

'
[1731156369] Input prefix: '" "'
[1731156369] sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
[1731156369] generate: n_ctx = 2048, n_batch = 512, n_predict = -1, n_keep = 1
[1731156369] 

[1731156369] == Running in interactive mode. ==
[1731156369]  - Press Ctrl+C to interject at any time.
[1731156369]  - Press Return to return control to LLaMa.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.

[1731156369] embd_inp.size(): 1, n_consumed: 0
[1731156369] eval: [ '':1 ]
[1731156380] n_past = 1
[1731156380] embd_inp.size(): 1, n_consumed: 1
[1731156380] waiting for user input
[1731156380] appending input prefix: '" "'
[1731156380] buffer: '" "Привет! У меня возле дома сломался фонарь '
[1731156380] inserting instruction prefix
[1731156380] input tokens: [ ' "':376, ' "':376, '':30013, '':641, '':7616, '!':29991, ' ':2014, ' ':26831, '':29970, ' ':6849, '':753, ' ':19794, ' ':19945, '':1155, '':4633, ' ':1606, '':2953, '':7474, ' ':29871 ]
[1731156380] inserting instruction suffix
[1731156380] n_remain: -20
[1731156380] embd_inp.size(): 40, n_consumed: 1
[1731156380] eval: [ '':1, ' ':29871, '':13, '':13, '##':2277, '#':29937, ' Inst':2799, 'ruction':4080, ':':29901, '':13, '':13, ' "':376, ' "':376, '':30013, '':641, '':7616, '!':29991, ' ':2014, ' ':26831, '':29970, ' ':6849, '':753, ' ':19794, ' ':19945, '':1155, '':4633, ' ':1606, '':2953, '':7474, ' ':29871, ' ':29871, '':13, '':13, '##':2277, '#':29937, ' Response':13291, ':':29901, '':13, '':13 ]
[1731156400] n_past = 40
[1731156400] embd_inp.size(): 40, n_consumed: 40
[1731156400] waiting for user input
[1731156400] appending input prefix: '" "'
